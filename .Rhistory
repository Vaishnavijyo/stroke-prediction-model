library(readr)
healthcare_dataset_stroke_data <- read_csv("healthcare-dataset-stroke-data.csv")
View(healthcare_dataset_stroke_data)
View(healthcare_dataset_stroke_data)
installed <- installed.packages()
installed[, "tidyverse"]
installed.packages()
# Define the packages you want to check
packages_to_check <- c("tidyverse", "caret", "randomForest", "e1071", "pROC", "shiny")
# Check installed packages
installed <- installed.packages()[, "Package"]
# Find which of the desired packages are installed
installed_status <- packages_to_check %in% installed
# Display the result
data.frame(Package = packages_to_check, Installed = installed_status)
# Load the dataset
stroke_data <- read.csv("healthcare-dataset-stroke-data.csv")
# View the first few rows of the data
head(stroke_data)
# Check the structure of the data
str(stroke_data)
# Get summary statistics
summary(stroke_data)
# Check for missing values
sum(is.na(stroke_data))
stroke_data$gender <- as.factor(stroke_data$gender)
stroke_data$smoking_status <- as.factor(stroke_data$smoking_status)
stroke_data$ever_married <- as.factor(stroke_data$ever_married)
stroke_data$work_type <- as.factor(stroke_data$work_type)
stroke_data$Residence_type <- as.factor(stroke_data$Residence_type)
stroke_data$stroke <- as.factor(stroke_data$stroke)
stroke_data$gender <- as.factor(stroke_data$gender)
stroke_data$smoking_status <- as.factor(stroke_data$smoking_status)
stroke_data$ever_married <- as.factor(stroke_data$ever_married)
stroke_data$work_type <- as.factor(stroke_data$work_type)
stroke_data$Residence_type <- as.factor(stroke_data$Residence_type)
stroke_data$stroke <- as.factor(stroke_data$stroke)
# Handle missing values in the BMI column by replacing with median
stroke_data$bmi[is.na(stroke_data$bmi)] <- median(stroke_data$bmi, na.rm = TRUE)
# Check for class imbalance
table(stroke_data$stroke)
q()
# Split the data into training and testing datasets
set.seed(123)
trainIndex <- createDataPartition(stroke_data$stroke, p = 0.8, list = FALSE)
library(caret)
exists("createDataPartition")
set.seed(123)
trainIndex <- createDataPartition(stroke_data$stroke, p = 0.8, list = FALSE)
trainData <- stroke_data[trainIndex, ]
testData <- stroke_data[-trainIndex, ]
# Train a Random Forest model
model_rf <- randomForest(stroke ~ ., data = trainData, ntree = 100)
library(randomForest)
exists("randomForest")
library(tidyverse)
library(caret)
library(randomForest)
library(e1071)
library(pROC)
library(shiny)
# Split the data into training and testing datasets
set.seed(123)
trainIndex <- createDataPartition(stroke_data$stroke, p = 0.8, list = FALSE)
trainData <- stroke_data[trainIndex, ]
testData <- stroke_data[-trainIndex, ]
# Train a Random Forest model
model_rf <- randomForest(stroke ~ ., data = trainData, ntree = 100)
# Print the model summary
print(model_rf)
# Task Three: Evaluate and Select Prediction Models
# Load the necessary libraries
library(caret)  # For confusionMatrix
library(pROC)   # For ROC and AUC
# Make predictions on the test data
predictions <- predict(model, testData)
# Train the Random Forest model on the training data
set.seed(123)
model <- randomForest(stroke ~ ., data = trainData, ntree = 100)
# Load the necessary libraries
library(caret)  # For confusion Matrix
library(pROC)   # For ROC and AUC
# Make predictions on the test data
predictions <- predict(model, testData)
# Confusion Matrix to evaluate model performance
conf_matrix <- confusionMatrix(predictions, testData$stroke)
print(conf_matrix)
# ROC Curve Analysis
roc_curve <- roc(testData$stroke, as.numeric(predictions))
plot(roc_curve, main = "ROC Curve for Stroke Prediction Model", col = "blue")
# Calculate AUC (Area Under the Curve)
auc_value <- auc(roc_curve)
print(paste("AUC:", auc_value))
---
title: "Build and deploy a stroke prediction model using R"
# Task Four: Deploy the prediction model with Shiny
library(shiny)
# Load the trained model
model_rf <- readRDS("stroke_prediction_model.rds")
getwd()
setwd("C:/Users/vyshn/OneDrive/Desktop/predictive storkes analysis")
setwd("C:/Users/vyshn/OneDrive/Desktop/predictive storkes analysis")
# Task Two: Build prediction models
# Split the data into training and testing datasets
set.seed(123)
trainIndex <- createDataPartition(stroke_data$stroke, p = 0.8, list = FALSE)
library(caret)
# Task Two: Build prediction models
# Split the data into training and testing datasets
set.seed(123)
trainIndex <- createDataPartition(stroke_data$stroke, p = 0.8, list = FALSE)
trainData <- stroke_data[trainIndex, ]
testData <- stroke_data[-trainIndex, ]
# Train a Random Forest model
library(randomForest)
model_rf <- randomForest(stroke ~ ., data = trainData, ntree = 100)
# Print the model summary
print(model_rf)
# Save the trained model for later use
saveRDS(model_rf, "stroke_prediction_model.rds")
list.files()
model_rf <- readRDS("stroke_prediction_model.rds")
getwd()  # Check your current working directory
library(caret)
trainIndex <- createDataPartition(stroke_data$stroke, p = 0.8, list = FALSE)
